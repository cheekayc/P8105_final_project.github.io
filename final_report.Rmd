---
title: "Final Report"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE,
  fig.width = 18,
  fig.height = 8,
  out.width = "90%",
  set.seed(8))

library(tidyverse)
library(forecast)
library(astsa)
library(MLmetrics)
library(lubridate)
library(plotly)
library(mgcv)
library(moments)
library(modelr)
```

## Project title
A French Bakery’s Daily Sales in 2021-2022
<p>&nbsp;</p >


## Data collection and cleaning
[kaggle](https://www.kaggle.com/datasets/matthieugimbert/french-bakery-daily-sales)

[price comparison](https://www.france-hotel-guide.com/en/blog/cost-baguette-paris/)
<p>&nbsp;</p >

We used updated version dataset of a French bakery from 2021-01-01 to 2022-09-30. The dataset provides the daily transaction details of customers. Since the dataset is still updating, we used downloaded version on 2022-12-03.

The dataset includes:

`date`: date order

`time`: time order

`ticket number`: identifier for every single transaction

`article`: name of the product sold (in French)

`quantity`: quantity sold

`unit_price`: price per product (in Euro)

`objective`: forecast the sales in order to ease the production planning

It contains 7 variables and 234000 observations.

<p>&nbsp;</p >
Firstly, we used `read_csv()` to import the dataset "Bakery_sales.csv", and applied `janitor::clean_names()` to change the variables' names into lower cases. Next, we tidied dataset by applying `mutate()` to remove the euro sign "€", change "," to ".", convert `unit_price` to numeric form, and renamed the `article` as `product_name`. Finally, we deleted the missing data via `filter()` and `article` via `select()`. We created an `revenue` by `unit_price` * `quantity`.
<p>&nbsp;</p >

## Motivation 
It is 10:05 am in the morning, you sit in the lecture hall having your earliest lecture of the day Data Science, empty-stomached. 
A cup of coffee and a nicely warmed chocolate croissant or as French says ‘Pain Au Chocolat’ is the only thing existing in your mind… 
You wonder why the bakery items are so good and what you can do with it accompanied with the ongoing endless talking of data science 
explained by your energetic professor… Well, you jumped online and searched datasets for a French bakery and here it happens…

Bread is so much a part of French culture that even the word for “friend” comes from Latin cum pane (with bread) meaning the person 
with whom you break bread. These everyday bakery items can still be something of a mystery to people like you and me (I mean, who doesn’t 
like carbs?)

In this journey, we will take you around the analysis of a typical French bakery and find out what’s the best-seller, how much they sell, 
how is the sale distributed and so much more… 
<p>&nbsp;</p >


## Related work
<p>&nbsp;</p >

## Initial questions
<p>&nbsp;</p >

## Exploratory analyses
<p>&nbsp;</p >

## Forecast modeling
<p>&nbsp;</p >
We will be testing four forecast models and identify the best model to help us predict the daily sale for this French bakery. 

<p>&nbsp;</p >
Sales forecast is important because it can be used as a daily production reference and help the bakery to operate smoothly and reduce wasting of resources, 
which in turn can reduce costs and increase profit.

<p>&nbsp;</p >
Time series forecasting involves using historical, time-stamped data to make predictions of what might happen in the future.

<p>&nbsp;</p >
## Summary of packages
tidyverse, forecast, astsa, MLmetrics, lubridate

```{r original dataset}
# Load and clean dataset:

bakery_df = 
  read_csv("./Data/Bakery_sales.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    unit_price = str_replace(unit_price, "€", ""),
    unit_price = str_replace(unit_price, ",", "."),
    unit_price = as.numeric(unit_price),
    year = year(date),
    month = month(date),
    hour = hour(time),
    product_name = article) %>% 
  filter(product_name != ".") %>% 
  select(-article)
```

In 2021, how many products were sold each day?
```{r 2021 daily sales df}
sale_2021 = 
  bakery_df %>% 
  filter(year == 2021) %>% 
  select(date, year, month, hour, product_name, quantity, unit_price) %>% 
  group_by(date) %>% 
  summarize(
    total_sale = sum(quantity))

sale_2021 %>% 
  head(10) %>% 
  knitr::kable(digits = 2)
```

The `sale_2021` dataset shows the number of products sold each day in 2021. In 2021, the bakery opened for business for 339 days. 

The line plot below shows the total number of products sold each day in 2021.

```{r 2021 daily sales line plot}
sale_2021 %>% 
  ggplot(aes(x = date, y = total_sale)) +
  geom_line(aes(color = "Orange")) +
  scale_x_date(date_labels = "%b %Y", date_breaks  = "1 month") +
  labs(
    x = "Date",
    y = "Number of products sold",
    title = "Bakery's Daily Sales (2021)") +
  theme_minimal() +
  theme(legend.position = "none") +
  theme(
    axis.text = element_text(size = 15),
    axis.title = element_text(size = 17, face = "bold"),
    plot.title = element_text(hjust = 0.4, size = 19, face = "bold"))
```

We will see this trend again as we verify the accuracy of different forecast models in the next few sections.

<p>&nbsp;</p>
### Data Pre-processing

We need to create a simple and clean dataframe derived from the original dataset `bakery_df` that can be used by the forecast models. This new dataframe only needs two columns:                          
* `date` : date order in which the bakery opened for business                                                                                                                                               
* `total_sale` : sum of products sold 

The `sales_all` dataframe consists of 600 rows, in which the dates begin with *January 2, 2021* and end with *September 30, 2022*. 

```{r simplify df}
sales_all = 
  bakery_df %>% 
  group_by(date) %>% 
  summarize(
    total_sale = sum(quantity))
```

Then, we need to create a `train` and a `test` data frames so that we can test the accuracy of our forecast models. We will be using all data prior to *September 1, 2022* as our `train` dataset 
to predict the amount of products sold daily for September, 2022. The remaining data from *September 1, 2022* to *September 30, 2022* will be our `test` dataset.

```{r train test df}
train = 
  sales_all %>% 
  filter(date < "2022-09-01")

test = 
  sales_all %>% 
  filter(year(date) == 2022 & month(date) == 9)
```

<p>&nbsp;</p>
#### Mean Absolute Percentage Error (MAPE)

For model evaluation, we will be relying on the ***Mean Absolute Percentage Error (MAPE)*** to measure the accuracy of our predictions. *MAPE* is a numeric value which ranges between 0 to 1.

The accuracy of the model can be calculated as $(1 - MAPE) * 100%$. For example, an *MAPE* value of 0.7 indicates the model has an accuracy of 30%.

We will report each model's accuracy using inline R code.

<p>&nbsp;</p>
### Seasonal Naive Model

First, we begin with the most basic forecast model - the **Seasonal Naive Model**.

Naive forecasting is a simple and cost-effective method in which the forecasts produced are equal to the last observed value. The seasonal naive approach is used 
when the time series exhibits seasonality, in which case, the forecasts are equivalent to the value from the last season. Naive methods are typically used as a 
benchmark against which more sophisticated forecasting techniques can be compared. 

```{r SNM}
# Fit the model
seasonal_naive_model = snaive(train$total_sale, h = length(test$total_sale))

# Compute error of the model
SNM_error = MAPE(seasonal_naive_model$mean, test$total_sale) * 100
```

**The accuracy of the *Seasonal Naive* model is `r round(sum(SNM_error = 100 - SNM_error), digits = 2)`%.**

The *Seasonal Naive* model will output a list of mean values, and they are the predicted values of daily sales from September 1, 2022 to September 30, 2022.
We need to extract the list of mean values from the model and put them into the `test` dataframe so that we can produce a time-series forecast plot.

```{r SNM predict}
test_seasonal = 
  test %>% 
  mutate(
    pred_sale = seasonal_naive_model$mean)
```

Finally, we can plot a time-series forecasting graph to show the results of our *Seasonal Naive* model.

```{r plot SNM prediction}
train %>% 
  ggplot(aes(x = date, y = total_sale)) +
  geom_line(aes(color = "Actual sale (Prior to 2022-09-01)")) +
  geom_line(data = test_seasonal, aes(x = date, y = total_sale, color = "Actual sale (2022-09-01 to 2022-09-30)")) + 
  geom_line(data = test_seasonal, aes(x = date, y = pred_sale, color = "Predicted sale (2022-09-01 to 2022-09-30)"), size = 1.5) +
  scale_x_date(date_labels = "%b %Y", date_breaks  = "2 month") +
  labs(
    x = "Date",
    y = "Number of products sold",
    title = "Seasonal Naive Forecast of Daily Sales for September (2022)") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  theme(
    axis.text = element_text(size = 15),
    axis.title = element_text(size = 17, face = "bold"),
    plot.title = element_text(hjust = 0.4, size = 19, face = "bold"),
    legend.text = element_text(size = 17),
    legend.title = element_text(size = 17))
```

<p>&nbsp;</p>
Comments: Although the *Seasonal Naive Model* gives high accuracy, the prediction line is a horizontal straight line, which means this model predicted that 
the amount of products sold is the same for each day. Therefore, we think that the predictions from this model are not the best.

<p>&nbsp;</p>
### Double-Seasonal Holt-Winters (DSHW) Model

The **Double-seasonal Holt-Winters** method uses additive trend and multiplicative seasonality, where there are two seasonal components which are multiplied together. 
The length of the two seasonalities must be multiples of one another (2 and 4, 4 and 12, etc.). In our case, we will specify the length of the first seasonality as 7 and the second as 14.

```{r DSHWM}
# Fit the model
double_seasonal_model = dshw(train$total_sale, period1 = 7, period2 = 14, h = length(test$total_sale))

# Compute the model's error
DSHW_error = MAPE(double_seasonal_model$mean, test$total_sale) * 100
```

**The accuracy of the *Double-seasonal Holt-Winters* model is `r round(sum(DSHW_error = 100 - DSHW_error), digits = 2)`%.**

The *DSHW* model will output a list of mean values, and they are the predicted values of daily sales from September 1, 2022 to September 30, 2022.
We need to extract the list of mean values from the model and put them into the `test` dataframe so that we can produce a time-series forecast plot.

```{r DSHW predict}
test_double_seasonal = 
  test %>% 
  mutate(
    pred_sale = double_seasonal_model$mean)
```

Finally, we can plot a time-series forecasting graph to see the results of our *DSHW* model.

```{r DSHW prediction plot}
train %>%
  filter(date > "2022-01-01") %>% 
  ggplot(aes(x = date, y = total_sale)) +
  geom_line(aes(color = "Actual sale (Prior to 2022-09-01)")) +
  geom_line(data = test_double_seasonal, aes(x = date, y = total_sale, color = "Actual sale (2022-09-01 to 2022-09-30)")) + 
  geom_line(data = test_double_seasonal, aes(x = date, y = pred_sale, color = "Predicted sale (2022-09-01 to 2022-09-30")) +
  scale_x_date(date_labels = "%b %Y", date_breaks  = "2 month") +
  labs(
    x = "Date",
    y = "Number of products sold",
    title = "Double-Seasonal Holt-Winters Forecast of Daily Sales for September (2022)") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  theme(
    axis.text = element_text(size = 15),
    axis.title = element_text(size = 17, face = "bold"),
    plot.title = element_text(hjust = 0.4, size = 19, face = "bold"),
    legend.text = element_text(size = 17),
    legend.title = element_text(size = 17))
```

<p>&nbsp;</p>
Comments: Although the *DSHW* model has a worse accuracy than the basic model (*Seasonal Naive*), but visually it is better than the basic model because it shows dynamic trend.
This would make more sense than the predictions given by the *Seasonal Naive* model as we can see that it is no longer predicting that the daily sales are the same for each day.

<p>&nbsp;</p>
### TBATS Model

**TBATS** is an acronym derived from the **Trigonometric seasonality, Box-Cox transformation, ARMA errors, Trend, and Seasonal components** of this approach. It takes its roots 
from exponential smoothing methods and is capable of modeling time series with multiple seasonalities.

```{r tbats model}
# Train a TBATS model
TBATS_model = tbats(train$total_sale)

# Generate forecast with the model
tbats_df = forecast(TBATS_model, h = length(test$total_sale))

# Check the error for the model
TBATS_error = MAPE(tbats_df$mean, test$total_sale) * 100
```

**The accuracy of the *TBATS* model is `r round(sum(TBATS_error = 100 - TBATS_error), digits = 2)`%.**

The *TBATS* model will output a list of mean values, and they are the predicted values of daily sales from September 1, 2022 to September 30, 2022.
We need to extract the list of mean values from the model and put them into the `test` dataframe so that we can produce a time-series forecast plot.

```{r TBATS predict}
test_tbats = 
  test %>% 
  mutate(
    pred_sale = tbats_df$mean)
```

```{r TBATS plot}
train %>% 
  filter(date > "2022-01-01") %>% 
  ggplot(aes(x = date, y = total_sale)) +
  geom_line(aes(color = "Actual sale (Prior to 2022-09-01)")) +
  geom_line(data = test_tbats, aes(x = date, y = total_sale, color = "Actual sale (2022-09-01 to 2022-09-30)")) + 
  geom_line(data = test_tbats, aes(x = date, y = pred_sale, color = "Predicted sale (2022-09-01 to 2022-09-30"), size = 0.8) +
  scale_x_date(date_labels = "%b %Y", date_breaks  = "2 month") +
  labs(
    x = "Date",
    y = "Number of products sold",
    title = "TBATS Forecast of Daily Sales for September (2022)") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  theme(
    axis.text = element_text(size = 15),
    axis.title = element_text(size = 17, face = "bold"),
    plot.title = element_text(hjust = 0.4, size = 19, face = "bold"),
    legend.text = element_text(size = 17),
    legend.title = element_text(size = 17))
```

<p>&nbsp;</p>
Comments: As we can see from the plot, the prediction line (blue) demonstrates dynamic pattern as the *DSHW* model, but the accuracy of the *TBATS* model is slightly better than the *DSHW* model.

<p>&nbsp;</p>
### Neural Network Model

A neural network is a series of algorithms that identifies patterns and relationships in data, similar to the way the brain operates. 

The `forecast` library comes with the option of a feed-forward neural network with a single hidden layer and lagged inputs for univariate time series forecasting.

```{r neural network model}
# Train a neural network model
nn_model = nnetar(train$total_sale)

# Generate forecast with the model
nn_forecast_df = forecast(nn_model, h = length(test$total_sale))

# Check the MAPE
NN_error = MAPE(nn_forecast_df$mean, test$total_sale) * 100
```

**The accuracy of the *Neural Network* model is `r round(sum(NN_error = 100 - NN_error), digits = 2)`%.**

The *Neural Network* model will output a list of mean values, and they are the predicted values of daily sales from September 1, 2022 to September 30, 2022.
We need to extract the list of mean values from the model and put them into the `test` dataframe so that we can produce a time-series forecast plot.

```{r NN prediction}
test_nn = 
  test %>% 
  mutate(
    pred_sale = nn_forecast_df$mean)
```

```{r NN plot}
train %>% 
# To show the prediction line more clearly, we are only showing the trends of daily sales after May 1, 2022.
  filter(date > "2022-05-01") %>% 
  ggplot(aes(x = date, y = total_sale)) +
  geom_line(aes(color = "Actual sale (Prior to 2022-09-01)")) +
  geom_line(data = test_nn, aes(x = date, y = total_sale, color = "Actual sale (2022-09-01 to 2022-09-30)"), alpha = 0.8) + 
  geom_line(data = test_nn, aes(x = date, y = pred_sale, color = "Predicted sale (2022-09-01 to 2022-09-30)"), size = 0.7, alpha = 0.8) +
  scale_x_date(date_labels = "%b %Y", date_breaks  = "1 month") +
  labs(
    x = "Date",
    y = "Number of products sold",
    title = "Neural Network Forecast of Daily Sales for September (2022)") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  theme(
    axis.text = element_text(size = 15),
    axis.title = element_text(size = 17, face = "bold"),
    plot.title = element_text(hjust = 0.4, size = 19, face = "bold"),
    legend.text = element_text(size = 17),
    legend.title = element_text(size = 17))
```

<p>&nbsp;</p>
Comments: As shown in the plot, the predicted trendline (blue) looks similar to the actual trendline (red) and highly overlaps it. Although the prediction accuracy is not 100%,
but this <mark >**Neural Network**</mark> model gives us the highest accuracy among all forecast models that has been tested.

<p>&nbsp;</p>
### Sales Forecast for Oct - Dec 2022

As we have identified that the <mark >**Neural Network**</mark>  model gives us the **highest accuracy** (`r round(sum(NN_error = 100 - NN_error), digits = 2)`%), we will use this model to predict the future daily sales 
from October to December in 2022.

This time, we will use all the data from January 1, 2021 to September 30, 2022 to fit into the *Neural Network* forecasting model to predict the daily sales of October - December in 2022.

```{r future prediction}
# Train the model
nn_future = nnetar(sales_all$total_sale)

# Generate the forecast
nn_future_df = forecast(nn_future, h = 92)
```

Since predicted values given by the model do not comes with dates, we need to create a dataframe with specific range of date to combine with the predicted values.

```{r date df}
dates = seq(as.Date('2022-10-01'), as.Date('2022-12-31'), by = 'days')

date_df =
  tibble(
    A = letters[sample(1:26, 92, TRUE)])

date_df = 
  tibble(
    date = rep(dates, length.out = nrow(date_df)))

date_df =
  date_df %>% 
  mutate(
    pred_sale = nn_future_df$mean)
```

Then, we can plot a graph to show the predicted daily sales of the bakery from October to December in 2022. 

```{r predicted future sales}
date_df %>% 
  ggplot(aes(x = date)) +
  geom_line(aes(y = pred_sale, color = "Red")) +
  scale_x_date(date_labels = "%b %d", date_breaks  = "10 day") +
  labs(
    x = "Date (Month Day)",
    y = "Number of products sold each day",
    title = "Predicted Daily Sales from October to December (2022)") +
  theme_minimal() +
  theme(legend.position = "none") +
  theme(
    axis.text = element_text(size = 15),
    axis.title = element_text(size = 17, face = "bold"),
    plot.title = element_text(hjust = 0.4, size = 19, face = "bold"),
    legend.text = element_text(size = 17),
    legend.title = element_text(size = 17))
```

<p>&nbsp;</p >
## Descriptions of approaches
<p>&nbsp;</p >
We will perform several statistical analysis in this bakery including ANOVA to check the mean difference of peak sale months between 2021 and 2022, One-Sample t-test to check the mean price of baguette with average price in the nation, simple linear regression to test the linear relationship between quantity of sale and hours in the day, and finally multiple linear regression.

## Discussion of results
<p>&nbsp;</p >
### ANOVA
```{r, warning = FALSE, message=FALSE}
# Data input and cleaning 
bakery_df =
  read_csv("./Data/Bakery_sales.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    unit_price = str_replace(unit_price, "€", ""),
    unit_price = str_replace(unit_price, ",", "."),
    unit_price = as.numeric(unit_price),
    product_name = article, 
    rev = quantity * unit_price
    ) %>% 
  filter(product_name != ".") %>% 
  select(-article)

```
ANOVA tests whether there is a difference in means of the groups at each level (each individual month) of the independent variable of peak months (Jun - Sep).

One way anova test can test if the mean sales of peak months (Jun - Sep) in 2021 is different from the mean sales of peak months (Jun - Sep) in 2022.

Our data contains 7 variables and 234000 observations, and we can assume the large sample size being normally distributed by central limit theorem. (The central limit theorem (CLT) states that the distribution of sample means approximates a normal distribution as the sample size gets larger(n>30), regardless of the population's distribution.) 

The null hypothesis is that there is no difference in the mean sales of peak months in 2021 and 2022.                                        
The alternative hypothesis is that the means are different from one another.

```{r ANOVA, warning = FALSE, message = FALSE}
anova_df =
  bakery_df %>% 
  mutate(
    year = year(date),
    month = month(date)
    ) 
  

one_sales = 
  anova_df %>% 
  filter((month == 6)|(month == 7)|(month == 8)|(month == 9)) %>% 
  filter(year == 2021) %>% 
  group_by(year, month) %>% 
  summarize(one_sales = n()) %>% 
  group_by(year, month) %>% 
  mutate(ID = cur_group_id())
  
two_sales = 
  anova_df %>% 
  filter((month == 6)|(month == 7)|(month == 8)|(month == 9)) %>% 
  filter(year == 2022) %>% 
  group_by(year, month) %>% 
  summarize(two_sales = n()) %>% 
  group_by(year, month) %>% 
  mutate(ID = cur_group_id())
  
anova_test_df = 
  left_join(one_sales, two_sales, by = c("ID"))


one.way <- aov(one_sales ~ two_sales, data = anova_test_df) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 2) 

one.way
```
The ANOVA test p-value is 0.01 which is less than alpha level of 0.05, so we reject the null hypothesis and conclude that the mean sales of peak months in 2021 is statistically significantly different from the mean sales of peak months in 2022. According to the Exploratory Analysis graphs, 2022 has increased the overall sales compared to 2021. This might due to the reasons of COVID-19 pandemic during 2021 and a better recovery with less restricted rules in 2022, which makes a significant difference in same period of the two years.
 
<p>&nbsp;</p >
### One Sample T-test
We are interested in testing if the mean price of `BAGUETTE` in this bakery is significantly different from the average price for a baguette in Paris, which is 1.07 euros. 

<p>&nbsp;</p>
<img src="images/baguette.gif" style="width:100%">

Null hypothesis: The mean price of baguette in this bakery is the same as the average price of baguette in Paris.

Alternative hypothesis: The mean price of baguette in this bakery is different from the average price of baguette in Paris.

Our population follows a Poisson distribution as shown below, and we can assume the large sample size being normally distributed by central limit theorem. 

```{r, warning = FALSE, message = FALSE}
bakery_df %>% 
  filter(product_name == "TRADITIONAL BAGUETTE") %>% 
  group_by(date) %>% 
  summarize(total_sale = sum(quantity)) %>% 
  ggplot(aes(x = total_sale)) +
  geom_histogram() +
  labs(
    x = "Total sale quantity per day", 
    y = "Count",
    title = "Distribution of sale quantity") +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(hjust = 0.4, face = "bold"))

baguette_onet = 
  bakery_df %>% 
  filter(product_name == "BAGUETTE") %>%
  select(unit_price)

baguette_t_results = 
  t.test(baguette_onet, mu = 1.07 , alternative = "two.sided") %>% 
  broom::tidy() 

baguette_t_results %>% 
  knitr::kable(digits = 4)
```

The p-value is much smaller than the alpha (0.05), so we would reject the null hypothesis. At 5% level of significance, we have sufficient 
evidence to conclude that the mean price of baguette in this bakery is significantly different from the average price of baguette in Paris.

<p>&nbsp;</p >
### Traditional Baguette (One-sided)

We noticed that the price of traditional baguette in this bakery is higher than the average price of traditional baguette in France. The
price of the traditional French loaf is around 0.90 Euros in bakeries. Therefore, we would like to conduct a one-sided T-test to see if the price difference is significant. 

Null hypothesis: The mean price of traditional baguette in this bakery is the same as the average price of traditional baguette in France.

Alternative hypothesis: The mean price of traditional baguette in this bakery is higher than the average price of traditional baguette in
France.

```{r, warning = FALSE}
bakery_df %>% 
  filter(product_name == "TRADITIONAL BAGUETTE") %>% 
  count(unit_price) %>% 
   knitr::kable(digits = 4)
  
trad_baguette_onet = 
  bakery_df %>% 
  filter(product_name == "TRADITIONAL BAGUETTE") %>%
  select(unit_price)

trad_baguette_t_results = 
  t.test(trad_baguette_onet, mu = 0.90, alternative = "greater") %>% 
  broom::tidy() 

trad_baguette_t_results %>% 
   knitr::kable(digits = 4)
```

The p-value is much smaller than the alpha (0.05), so we would reject the null hypothesis. At 5% level of significance, we have sufficient 
evidence to conclude that the mean price of traditional baguette in this bakery is significantly higher than the average price of baguette in Paris.

<p>&nbsp;</p >
### Simple Linear Regression 
We conduct a simple linear regression for `CROISSANT`'s sale counts and when it's being sold (hour of the day) and test the model using `Cross Validation`.

<img src="images/croissant.gif" style="width:100%">

The plot below shows the average amount of products sold in each hour during the business hour.
```{r, warning = FALSE}
bakery_df = 
  bakery_df %>% 
  mutate(
    Hour = hour(time),
    Month = month(date))

bakery_df %>% 
  group_by(Hour) %>% 
  count() %>%
  mutate(
    average_count = n / 600) %>% 
  ggplot(aes(x = Hour, y = average_count)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq(7, 20), limit = c(7, 20)) +
  scale_y_continuous(limit = c(0, 80)) +
  labs(
    title = "Average amount of products sold in every hour per day",
    x = "Hour (24-hour format)",
    y = "Number of products sold") +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(hjust = 0.4, face = "bold"))
``` 

From the graph above, it seems like there is a linear trend from 7 AM to 11 AM. Hence, we should take a look at the morning hours with a monotonic function from 7 AM to 11 AM.
```{r, warning = FALSE}
bakery_df %>% 
  mutate(year = year(date)) %>% 
  filter(year == 2021) %>% 
  group_by(Hour) %>% 
  summarize(
    n_sold = sum(quantity) / 365) %>% 
  ggplot(aes(x = Hour, y = n_sold)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq(7, 11), limit = c(7, 11)) +
  scale_y_continuous(limit = c(0, 120)) +
  labs(
    title = "Average amount of products sold from 7am - 11am",
    x = "Hour (24-hour format)",
    y = "Number of products sold") +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(hjust = 0.4, face = "bold"))
```

It looks like a monotonic linear relationship that we can take a further look at using simple linear regression model.

```{r, warning = FALSE}
slr_df = 
  bakery_df  %>% 
  mutate(
    hour_cp = (Hour > 8) * (Hour - 8))

slr_reg = lm(quantity ~ Hour, slr_df)

slr_reg %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 2) 
```
The model is 
Sale count = 1.99 - 0.04 * Hour

<p>&nbsp;</p >
#### Cross validation 
```{r, warning = FALSE}
# created 3 models: linear, piecewise, and smooth models 
linear_mod = lm(quantity ~ Hour, slr_df)
pwl_mod    = lm(quantity ~ Hour + hour_cp, data = slr_df)
smooth_mod = gam(quantity ~ s(Hour), data = slr_df)
```

```{r, warning = FALSE}
slr_df %>% 
  gather_predictions(linear_mod, pwl_mod, smooth_mod) %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = Hour, y = quantity)) + 
  geom_point(alpha = .5, size = 0.05) +
  geom_line(aes(y = pred), color = "red") +
  ylim(1, 2) +
  facet_grid(~model)
```

The three models with original scales are very similar. Therefore, when we visualize the three models, we change the y-axis scale to 0-3 as a zoom-in function in order to see the nuance differences between each model. 


Re-sample the dataset by `crossv_mc` and let's see the rmse of each model and make a table
```{r, warning = FALSE}
# created a train model and test model; check rmse 
set.seed(2022)
cv_df =
  crossv_mc(slr_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(
    linear_mod  = map(train, ~lm(quantity ~ Hour, data = .x)),
    pwl_mod     = map(train, ~lm(quantity ~ Hour + hour_cp, data = .x)),
    smooth_mod  = map(train, ~gam(quantity ~ s(Hour), data = as_tibble(.x)))) %>% 
  mutate(
    rmse_linear = map2_dbl(linear_mod, test, ~rmse(model = .x, data = .y)),
    rmse_pwl    = map2_dbl(pwl_mod, test, ~rmse(model = .x, data = .y)),
    rmse_smooth = map2_dbl(smooth_mod, test, ~rmse(model = .x, data = .y))) 

  cv_df %>% 
  select(rmse_linear, rmse_pwl, rmse_smooth) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  group_by(model) %>% 
  summarize(mean(rmse)) %>% 
  mutate(mean_rmse = `mean(rmse)`) %>% 
  select(-`mean(rmse)`) %>% 
  knitr::kable(digits = 4)
```
Then plot the rmse graph
```{r, warning = FALSE}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```
From the visualization graphs of three rmse models, it is hard to determine the slightly change of each model, but from our mean rmse table we conclude that the smooth model is the best out of the three with the least mean rmse (1.2688).

Based on the results, there is a slightly improvement using smooth model as it has the lowest rmse, we can conclude that the smooth model is to be accounted for this relationship in the product of croissant.

<p>&nbsp;</p >
### Multiple Linear Regression

We are interested in testing the relationship between the revenue (outcome variable) and 2 main predictors, which are unit price of a product and its quantity being sold. Considering that the sales of bakery varies by month, we included `month` in our model as a confounder to be controlled.


<p>&nbsp;</p>

Multivariable linear regression model 
```{r, warning = FALSE}
baguette_df = 
  bakery_df %>% 
  filter(str_detect(product_name, "BAGUETTE")) %>% 
  mutate(
    month = month(date)
  )
baguette_reg = lm(rev ~ unit_price + quantity + month, baguette_df)


baguette_reg %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 4)
```

Examine assumptions for the chosen Linear Regression model

```{r, warning = FALSE}
par(mfrow = c(2, 2))
plot(baguette_reg)
```

According to the assumption check graph, the normal qq plot is approximately closed to the predicted line with residuals on both sides, but the residual plots might violate some of the linear regression assumptions (homoscedasticity). 

The tentative model for multiple linear regression of baguette's revenue is  
Revenue = -1.43 + 1.21 * unit_price + 1.19 * quantity + 0.001 * month

We might not consider doing MLR for the sake of this research due to the unmet assumptions.

<p>&nbsp;</p >
## Strengths and Limitations
<p>&nbsp;</p >

## Finding, Summary, and Expectation
<p>&nbsp;</p >

