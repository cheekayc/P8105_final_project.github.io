---
title: "Statistical Analyses"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE,
  fig.width = 14,
  fig.height = 6,
  out.width = "90%",
  set.seed(8))

library(tidyverse)
library(lubridate)
library(plotly)
library(mgcv)
library(moments)
library(modelr)
```

Data input and cleaning 
```{r, warning = FALSE}
bakery_df =
  read_csv("./Data/Bakery_sales.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    unit_price = str_replace(unit_price, "â‚¬", ""),
    unit_price = str_replace(unit_price, ",", "."),
    unit_price = as.numeric(unit_price),
    product_name = article, 
    rev = quantity * unit_price
    ) %>% 
  filter(product_name != ".") %>% 
  select(-article)

```

### ANOVA

ANOVA tests whether there is a difference in means of the groups at each level (each individual month) of the independent variable of peak months (Jun - Sep).

One way anova test can test if the mean sales of peak months (Jun - Sep) in 2021 is different from the mean sales of peak months (Jun - Sep) in 2022.

The null hypothesis is that there is no difference in the mean sales of peak months in 2021 and 2022.                                        
The alternative hypothesis is that the means are different from one another.

```{r ANOVA, warning = FALSE}
anova_df =
  bakery_df %>% 
  mutate(
    year = year(date),
    month = month(date)
    ) 
  

one_sales = 
  anova_df %>% 
  filter((month == 6)|(month == 7)|(month == 8)|(month == 9)) %>% 
  filter(year == 2021) %>% 
  group_by(year, month) %>% 
  summarize(one_sales = n()) %>% 
  group_by(year, month) %>% 
  mutate(ID = cur_group_id())
  
two_sales = 
  anova_df %>% 
  filter((month == 6)|(month == 7)|(month == 8)|(month == 9)) %>% 
  filter(year == 2022) %>% 
  group_by(year, month) %>% 
  summarize(two_sales = n()) %>% 
  group_by(year, month) %>% 
  mutate(ID = cur_group_id())
  
anova_test_df = 
  left_join(one_sales, two_sales, by = c("ID"))


one.way <- aov(one_sales ~ two_sales, data = anova_test_df)

summary(one.way)
```
The ANOVA test p-value is 0.00505 which is less than alpha level of 0.05, so we reject the null hypothesis and conclude that the mean sales of peak months in 2021 is statistically significantly different from the mean sales of peak months in 2022.
 
 
### Generalized Additive Model 

Find the *rush hours* of a typical day:
```{r, warning = FALSE}
bakery_df = 
  bakery_df %>% 
  mutate(
    Hour = hour(time),
    Month = month(date))

bakery_df %>% 
  group_by(Hour) %>% 
  count() %>% 
  ggplot(aes(x = Hour, y = n)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq(7, 20), limit = c(7, 20)) +
  scale_y_continuous(limit = c(0,50000)) +
  labs(
    title = "Peak hours",
    x = "Hour (24-hour format)",
    y = "Number of times appeared")
``` 

Overall, what hours has the most number of products sold?

Then we only take a look at the morning hours with a monotonic function from 7 AM to 11 AM.
```{r, warning = FALSE}
bakery_df %>% 
  mutate(year = year(date)) %>% 
  filter(year == 2021) %>% 
  group_by(Hour) %>% 
  summarize(
    n_sold = sum(quantity) / 365) %>% 
  ggplot(aes(x = Hour, y = n_sold)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq(7, 11), limit = c(7, 11)) +
  scale_y_continuous(limit = c(0,120)) +
  labs(
    title = "Peak hours",
    x = "Hour (24-hour format)",
    y = "Number of products sold")

```

It looks like a monotonic linear relationship that we can take a further look at using simple linear regression model.

### Simple Linear Regression for croissant's sale counts and when it's being sold (hour of the day) and test the model using Cross Validation
```{r, warning = FALSE}
slr_df = 
  bakery_df  %>% 
  mutate(
    hour_cp = (Hour > 8) * (Hour - 8)
    )

ggplot(slr_df, aes(x = Hour, y = quantity)) +
  geom_point() + scale_x_continuous(breaks = seq(7, 11), limit = c(7, 11)) +
  scale_y_continuous(limit = c(0,50)) 

slr_reg = lm(quantity ~ Hour, slr_df)

slr_reg %>% 
  broom::tidy()
```
The model is 
Sale count = 1.99 - 0.04 * Hour

Examine cross validation of this simple linear regression model
```{r, warning = FALSE}
linear_mod = lm(quantity ~ Hour, slr_df)
pwl_mod    = lm(quantity ~ Hour + hour_cp, data = slr_df)
smooth_mod = gam(quantity ~ s(Hour), data = slr_df)
```

```{r, warning = FALSE}
slr_df %>% 
  gather_predictions(linear_mod, pwl_mod, smooth_mod) %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = Hour, y = quantity)) + 
  geom_point(alpha = .5) +
  geom_line(aes(y = pred), color = "red") + 
  facet_grid(~model)
```

The three models are very similar.

Re-sample the dataset by `crossv_mc` and let's see the rmse of each model
```{r, warning = FALSE}
set.seed(2022)
cv_df =
  crossv_mc(slr_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(
    linear_mod  = map(train, ~lm(quantity ~ Hour, data = .x)),
    pwl_mod     = map(train, ~lm(quantity ~ Hour + hour_cp, data = .x)),
    smooth_mod  = map(train, ~gam(quantity ~ s(Hour), data = as_tibble(.x)))) %>% 
  mutate(
    rmse_linear = map2_dbl(linear_mod, test, ~rmse(model = .x, data = .y)),
    rmse_pwl    = map2_dbl(pwl_mod, test, ~rmse(model = .x, data = .y)),
    rmse_smooth = map2_dbl(smooth_mod, test, ~rmse(model = .x, data = .y)))

```
Then plot the rmse graph
```{r, warning = FALSE}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

Based on the results, there is a slightly improvement using smooth model as it has a lower tail of rmse on the top. Since there are no significant difference between the three model including the non-linear smooth model, we can conclude that the basic linear model is clear enough to be accounted for this relationship in the product of croissant.

### Multiple Linear Regression (test version)

We are interested in testing the relationship between the revune and unit price of a product and its quantity being sold. Considering that 
the sales of bakery varies by month, so we included `month` in our model as a confounder to be controlled.

<p>&nbsp;</p>
<img src="images/baguette.gif" style="width:100%">
<p>&nbsp;</p>

Linear Regression and check assumption for product baguette
```{r, warning = FALSE}
baguette_df = 
  bakery_df %>% 
  filter(str_detect(product_name, "BAGUETTE")) %>% 
  mutate(
    month = month(date)
  )
baguette_reg = lm(rev ~ unit_price + quantity + month, baguette_df)


baguette_reg %>% 
  broom::tidy()
```

Examine assumptions for the chosen Linear Regression model

```{r, warning = FALSE}
par(mfrow = c(2, 2))
plot(baguette_reg)
```

According to the assumption check graph, the normal qq plot is approximately closed to the predicted line with residuals on both sides, but the residual plots might violate the linear regression assumptions.

The tentative model for multiple linear regression of baguette's revenue is  
Revenue = -1.43 + 1.21 * unit_price + 1.19 * quantity + 0.001 * month

We might not consider doing MLR for the sake of this research due to the unmet assumptions.

### One-sample T-test

##### Regular Baguette (Two-sided)

We are interested in testing if the mean price of **regular baguette** in this bakery is significantly different from 
the average price for a baguette in Paris, which is 1.07 euros. 

Null hypothesis: The mean price of baguette in this bakery is the same as the average price of baguette in Paris.

Alternative hypothesis: The mean price of baguette in this bakery is different from the average price of baguette in Paris.

Our population follows a Poisson distribution as shown below, and we can assume the large sample size being normally distributed by central limit theorem. 

```{r, warning = FALSE}
bakery_df %>% 
  filter(product_name == "TRADITIONAL BAGUETTE") %>% 
  group_by(date) %>% 
  summarize(total_sale = sum(quantity)) %>% 
  ggplot(aes(x = total_sale)) +
  geom_histogram()

baguette_onet = 
  bakery_df %>% 
  filter(product_name == "BAGUETTE") %>%
  select(unit_price)

baguette_t_results = 
  t.test(baguette_onet, mu = 1.07 , alternative = "two.sided") %>% 
  broom::tidy()
```

The p-value is much smaller than the alpha (0.05), so we would reject the null hypothesis. At 5% level of significance, we have sufficient 
evidence to conclude that the mean price of baguette in this bakery is significantly different from the average price of baguette in Paris.


##### Traditional Baguette (One-sided)

We noticed that the price of traditional baguette in this bakery is higher than the average price of traditional baguette in France. The
price of the traditional French loaf is around 0.90 Euros in bakeries. Therefore, we would like to conduct a one-sided T-test to see if the price difference is significant. 

Null hypothesis: The mean price of traditional baguette in this bakery is the same as the average price of traditional baguette in France.

Alternative hypothesis: The mean price of traditional baguette in this bakery is higher than the average price of traditional baguette in
France.

```{r, warning = FALSE}
bakery_df %>% 
  filter(product_name == "TRADITIONAL BAGUETTE") %>% 
  count(unit_price)
  
trad_baguette_onet = 
  bakery_df %>% 
  filter(product_name == "TRADITIONAL BAGUETTE") %>%
  select(unit_price)

trad_baguette_t_results = 
  t.test(trad_baguette_onet, mu = 0.90, alternative = "greater") %>% 
  broom::tidy()
```

The p-value is much smaller than the alpha (0.05), so we would reject the null hypothesis. At 5% level of significance, we have sufficient 
evidence to conclude that the mean price of traditional baguette in this bakery is significantly higher than the average price of baguette 
in Paris.







```{r, warning = FALSE}
bakery_df %>%
  mutate(
    month = month(date),
    year = year(date)) %>% 
  filter(month == 2 & year == 2021) %>% 
  group_by(product_name) %>% 
  summarize(total_sale = sum(quantity)) %>% 
  filter(total_sale < 1000) %>% 
  ggplot(aes(x = total_sale)) +
  geom_histogram(bins = 71)

skew = 
bakery_df %>% 
  group_by(date) %>% 
  summarize(total_sale = sum(quantity))

# Skewness test
print(skewness(skew))
```

